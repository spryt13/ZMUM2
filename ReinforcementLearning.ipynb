{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95574da6-ff5a-4d7b-9462-a2f2cb5feb9c",
   "metadata": {},
   "source": [
    "# Projekt nr 2\n",
    "#### Maciej Konieczny\n",
    "#### Kamila Martyniuk-Sienkiewicz\n",
    "Na rzecz uruchomienia projektu, konieczne jest załadowanie środowiska i biblioteki agentów. Komendy w następnej komórce importują co potrzeba, ALE załadowanie środowiska Hoppera wymaga fizyki mujoco i biblioteki imageio:\n",
    "- pip install mujoco\n",
    "- pip install imageio\n",
    "\n",
    "Przy instalacji mujoco w wersji 3 lub nowszej wystepuje błąd uniemożliwiający wyświetlenie Hoppera dla użytkownika (tak, żeby sobie mógł popatrzeć). Do naprawy tego błędu trzeba znaleźć plik mujoco_rendering.py i zmienić nazwę zmiennej:\n",
    "- ścieżka to (od folderu z Anacondą) Anaconda3\\envs\\\"nazwa środowiska w anacondzie\"\\Lib\\site-packages\\gymnasium\\envs\\mujoco\n",
    "- w linijce nr 593 zmienić \"self.data.solver_iter\" na \"self.data.solver_niter\" (dopisać \"n\")\n",
    "- Debugger jupytera podpowiada powyższą ścieżkę wraz z lokalizacją w pliku."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1d16f-d5fd-4a42-9dac-779e821e312d",
   "metadata": {},
   "source": [
    "## Tworzenie środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6bd4eac-32b4-439c-8fa3-483926aadf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie bibliotek\n",
    "import os as os # biblioteka do tworzenia ścieżki zapisu i odczytu\n",
    "import gymnasium as gym # gymansium, baza środowisk\n",
    "from stable_baselines3 import PPO # agent PPO\n",
    "from stable_baselines3 import DDPG # agent DDPG\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # funkcja do ewaluacji agentów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e87c0e-50e8-4e63-98fa-31e4c3bec554",
   "metadata": {},
   "source": [
    "Załadowanie środowiska po raz pierwszy sprawia, że można wyświetlać to, co się tam dzieje i można w nim uczyć agentów. Jednak po zamknięciu środowiska nie będzie nic wyświetlane, dopóki nie zostanie załadowane od nowa. Wszystkie inne operacji m. in. uczenie nadal można przeprowadzać. Trzeba o tym pamiętać jeśli chce się oglądać agenta, ale też warto jest zamknąć środowisko przed rozpoczęciem uczenia, żeby nie wyświetlało każdego podejścia agenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d78914-ccdd-470d-bc25-23bb28a722fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załadowanie środowiska z gymnasium\n",
    "env = gym.make('Hopper-v4', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd4105ab-1845-4370-9850-8d390655c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zamknięcie środowiska\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576e6f6-2935-4aec-b3e1-95c866ac6d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test. Jeśli wszystko zostało poprawnie ustawione, uruchomi się środowisko z Hopperem wykonującym losowe akcje.\n",
    "observation, info = env.reset(seed=0)\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    frame = env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd5947-b2ac-4ba6-b46b-2743e3a8284d",
   "metadata": {},
   "source": [
    "## Tworzenie i uczenie agentów\n",
    "\n",
    "Parametr verbose powoduje pisanie loga podczas uczenia. W zależności od parametru total_timesteps, uczenie zajmie mniej lub więcej czasu, a wyświetlany log pokazuje ile takich jednostek czasu upłynęło, co można odebrać jako postęp uczenia (dla potrzeby szacowania *\"ile to jeszcze może zająć\"*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51707d-64c1-45c9-8b1c-d9896146bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stworzenie agenta i jego uczenie (agent PPO)\n",
    "agentPPO = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "agentPPO.learn(total_timesteps=200000) # UWAGA! Uczenie zajmuje dużo czasu! (u mnie 200000 zajęło od 10 do 15 minut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f6c85-a5b9-4ae2-9909-60f21a34b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stworzenie agenta i jego uczenie (agent DDPG)\n",
    "agentDDPG = DDPG(\"MlpPolicy\", env, verbose=1)\n",
    "agentDDPG.learn(total_timesteps=200000) # UWAGA! Uczenie zajmuje BARDZO dużo czasu! (u mnie 200000 zajęło ok. 5 godzin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea2c9d-192f-4633-afd6-954c3882bd26",
   "metadata": {},
   "source": [
    "## Zapisanie i ładowanie parametrów agentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840a9c2c-d5aa-4c1f-b0ee-cd5febe3eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie ścieżki zapisu (wg poniższego zapisu modele muszą znajdować się w folderze o nazwie \"agents\", który znajduje się obok tego Jupytera)\n",
    "pathPPO = os.path.join('agents', 'PPO')\n",
    "pathDDPG = os.path.join('agents', 'DDPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212bcde-bf40-443b-823b-dcf27e4b6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapis parametrów agentów\n",
    "agentPPO.save(pathPPO)\n",
    "agentDDPG.save(pathDDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81cb0721-c2c9-4b64-8b7a-162783f4efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Załadowanie parametrów agentów\n",
    "agentPPO = PPO.load(pathPPO, env=env)\n",
    "agentDDPG = DDPG.load(pathDDPG, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34098a5-540b-4be1-a8c3-d83bab335da8",
   "metadata": {},
   "source": [
    "## Testowanie i obserwacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "725bd87b-81bf-4dca-aa5f-fb7788ea02e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spryt\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2270.486060063044, 9.565386642580773)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testowanie/Ewaluacja agenta. Funkcja ewaluacji zwraca kolejno średnią nagrodę i odchylenie standardowe\n",
    "# agent PPO\n",
    "evaluate_policy(agentPPO, env, n_eval_episodes=3, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "626ec7c2-de4c-4bc9-8bf6-ec1c78505137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spryt\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\glfw\\__init__.py:916: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1098.071695381403, 613.2127775657158)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent DDPG\n",
    "evaluate_policy(agentDDPG, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a1b11-e8d5-4bc3-a126-b913de7f8a46",
   "metadata": {},
   "source": [
    "## Kod alternatywny do obserwacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d075b31f-a88b-4268-82ee-4462879c8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obserwacja agenta PPO\n",
    "observation, info = env.reset(seed=0)\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, observation = agentPPO.predict(observation, info) # wykorzystanie PPO do wykonania akcji\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    frame = env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f93b0-4402-4871-b87a-4dba200378e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obserwacja agenta DDPG\n",
    "observation, info = env.reset(seed=0)\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, observation = agentDDPG.predict(observation, info) # wykorzystanie DDPG do wykonania akcji\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    frame = env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
